cases in dim time
	- leap years
	- national holidays and special days

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, date_add, date_sub
from datetime import datetime, timedelta

spark = SparkSession.builder.appName("DimTimeTable").getOrCreate()

input_date = "2000-01-01"

input_date_dt = datetime.strptime(input_date, "%Y-%m-%d")
start_date = input_date_dt - timedelta(days=3650)
end_date = input_date_dt + timedelta(days=3650)

date_df = spark.range((start_date - timedelta(days=1)).strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")).toDF("Date")
date_df = date_df.dropDuplicates(["Date"])

min_date = date_df.selectExpr("MIN(Date)").first()[0]
max_date = date_df.selectExpr("MAX(Date)").first()[0]

full_date_range_df = spark.range(min_date, date_add(max_date, 1), timedelta(days=1)).toDF("Date")

date_df = full_date_range_df.join(date_df, "Date", "left").select("Date")

date_df.createOrReplaceTempView("date_df")

dim_time_sql = """
SELECT 
    ROW_NUMBER() OVER (ORDER BY Date) AS TimeSkey,
    Date,
    DATE_FORMAT(Date, 'EEEE') AS DayOfWeek,
    CASE WHEN Date = (SELECT MAX(Date) FROM date_df) THEN 1 ELSE 0 END AS CurrentDay,
    CASE WHEN DATE_FORMAT(Date, 'u') IN ('6', '7') THEN 0 ELSE 1 END AS WorkingDay,
    MONTH(Date) AS MonthId,
    DATE_FORMAT(Date, 'MMMM') AS MonthDesc,
    CAST(QUARTER(Date) AS SMALLINT) AS QuarterId,
    CONCAT('Q', QUARTER(Date)) AS QuarterDesc,
    YEAR(Date) AS Year
FROM date_df
"""

dim_time_df = spark.sql(dim_time_sql)

dim_time_df.show()




from pyspark.sql.functions import col, lit, current_date, concat, xxhash64

# Join sourceDF and targetDF
joinDF = sourceDF.join(targetDF, (sourceDF.pk1 == targetDF.pk1) & 
                       (sourceDF.pk2 == targetDF.pk2) & 
                       (targetDF.active_status == "Y"), "leftouter") \
    .select(sourceDF["*"], 
            targetDF.pk1.alias("target_pk1"), 
            targetDF.pk2.alias("target_pk2"), 
            targetDF.dim1.alias("target_dim1"), 
            targetDF.dim2.alias("target_dim2"), 
            targetDF.dim3.alias("target_dim3"), 
            targetDF.dim4.alias("target_dim4"))

display(joinDF)

# Filter rows where source and target hashes don't match
filterDF = joinDF.filter(xxhash64(joinDF.dim1, joinDF.dim2, joinDF.dim3, joinDF.dim4) != 
                         xxhash64(joinDF.target_dim1, joinDF.target_dim2, joinDF.target_dim3, joinDF.target_dim4))
display(filterDF)

# Create merge key
mergeDF = filterDF.withColumn("MERGEKEY", concat(filterDF.pk1, filterDF.pk2))
display(mergeDF)

# Create dummy DataFrame
dummyDF = filterDF.filter("target_pk1 is not null").withColumn("MERGEKEY", lit(None))
display(dummyDF)

scdDf = merged.union(dummyDf)

# Merge with target table
targetTable.alias("target").merge(
    source = scdDF.alias("source"),
    condition = "concat(target.pk1, target.pk2) = source.MERGEKEY and target.active_status = 'Y'"
).whenMatchedUpdate(set = {
    "active_status": lit(0),
    "end_date": current_date()
}).whenNotMatchedInsert(values = {
    "pk1": "source.pk1",
    "pk2": "source.pk2",
    "dim1": "source.dim1",
    "dim2": "source.dim2",
    "dim3": "source.dim3",
    "dim4": "source.dim4",
    "active_status": lit(1),
    "start_date": current_date(),
    "end_date": lit("9999-12-31")
}).execute()

